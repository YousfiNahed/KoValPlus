import os
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from setproctitle import setproctitle
import argparse
from tqdm import tqdm
import re
from openai import OpenAI
from huggingface_hub import login



# ================== 1. Functions ==================
def model_load(key, model_path):
    print(f"Loading model: {model_path}")

    if 'gpt' in model_path.lower():
        if not key:
            raise ValueError("OpenAI ëª¨ë¸ ì‚¬ìš© ì‹œ --key í•„ìš”í•©ë‹ˆë‹¤.")
        client = OpenAI(api_key=key)
        return client
    else:
        if key: login(token=key)    # HuggingFace Model
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            dtype="auto",
            device_map="auto"
        )
        return tokenizer, model

def create_messages(mode, item):
    if mode == "default":
        options_str = "\n".join([f"{k}. {v}" for k, v in item["Options"].items()])
        system_message_df = "You are acting as a survey respondent. From the given options, choose exactly one. Do not provide any explanation or extra text."
        messages = [
            {"role": "system", "content": system_message_df},
            {"role": "user", "content": f"Question: {item['Questions']}\nOptions: {options_str}"}
        ]
    else:
        options_str_kr = "\n".join([f"{k}. {v}" for k, v in item["Options_KR"].items()])
        system_message_kr = "ë‹¹ì‹ ì€ ì„¤ë¬¸ ì¡°ì‚¬ ì§€ì›ìì˜ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì œê³µëœ ì„ íƒì§€ ì¤‘ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì„ íƒí•˜ê³ , ì•„ë¬´ ì„¤ëª…ë„ í•˜ì§€ ë§ê³  ì„ íƒí•œ ë²ˆí˜¸ë§Œ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤. ë‹¹ì‹ ì´ í•œêµ­ì¸ì´ë¼ë©´ ì–´ë–»ê²Œ ë‹µë³€í• ì§€, í•œêµ­ì¸ì˜ ê´€ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤."
        messages = [
            {"role": "system", "content": system_message_kr},
            {"role": "user", "content": f"ì§ˆë¬¸: {item['Questions_KR']}\nì„ íƒì§€: {options_str_kr}"}
        ]

    return messages

def preprocess_response(options, response):
    def extract_first_number(r: str):
        # ë¬¸ìì—´ì—ì„œ ì²˜ìŒ ë‚˜ì˜¤ëŠ” ìˆ«ìë¥¼ ì°¾ì•„ intë¡œ ë°˜í™˜. ì—†ìœ¼ë©´ None
        match = re.search(r"\d+", r)
        if match:
            return int(match.group())
        return None

    """
    - ìŒìˆ˜ ì˜µì…˜ ì œì™¸
    - ìˆ«ìê°€ ìˆìœ¼ë©´ ìˆ«ì ì‚¬ìš©
    - ìˆ«ìê°€ ì—†ë”ë¼ë„ optionsì˜ value ê°’ê³¼ ê°™ìœ¼ë©´ í•´ë‹¹ keyë¡œ ë§¤í•‘
    - ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ None
    """
    pos_keys = [int(k) for k in options.keys() if int(k) > 0]
    if not pos_keys: return ""

    r_int = None
    # 1. ì •ìˆ˜ ë³€í™˜ ì‹œë„
    try:
        r_int = int(response)
    except ValueError:
        # 2. ë¬¸ìì—´ì—ì„œ ì²« ìˆ«ì ì¶”ì¶œ
        r_int = extract_first_number(response)

    # 3. ì˜µì…˜ value ê°’ê³¼ exact match í™•ì¸
    if r_int is None:
        matched = [k for k, v in pos_keys.items() if v.strip().lower() == str(response).strip().lower()]
        if matched:
            r_int = matched[0]

    # 4. ìœ íš¨ì„± ì²´í¬
    if r_int is None or int(r_int) not in pos_keys:
        return ""
    else:
        return r_int

def hf_chat(model, tokenizer, item, mode, max_new_tokens=10, n=30):
    # Input - Messages
    messages = create_messages(mode=mode, item=item)
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # Output - Generate N responses
    generated_ids = model.generate(
        **model_inputs, 
        max_new_tokens=max_new_tokens, 
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,             # ë‹¤ì–‘ì„± í™•ë³´ (ì¤‘ìš”)
        temperature=0.7,
        top_p=0.9,
        num_return_sequences=n
    )

    # Decode
    outputs = []
    prefix_len = model_inputs.input_ids.shape[1]
    for i in range(n):
        output_ids = generated_ids[i][prefix_len:].tolist()
        content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

        v = preprocess_response(options=item["Options"], response=content)
        if v != "": outputs.append(v)

    return outputs

def gpt_chat(client, item, mode, model_path):
    # Input - Messages
    messages = create_messages(mode=mode, item=item)

    # Output - Generate N responses
    _response = client.chat.completions.create(
        model=model_path,
        messages=messages,
        temperature=0.7,
        top_p=0.9,
        n=30
    )
    all_responses = [choice.message.content for choice in _response.choices]

    options = item["Options"]
    processed = [v for v in (preprocess_response(options, r) for r in all_responses) if v != ""]

    return processed

def run_survey(**kwargs):
    data        = kwargs["data"]                 # dict ë˜ëŠ” ë¡œë“œëœ JSON
    model_path  = kwargs["model_path"]           # ë¬¸ìì—´ (ì˜ˆ: "gpt-4o-mini" ë˜ëŠ” "meta-llama/...")
    mode        = kwargs.get("mode")             # í”„ë¡¬í”„íŒ… í‰ê°€ ëª¨ë“œ
    client      = kwargs.get("client")           # GPTìš©
    model       = kwargs.get("model")            # HFìš©
    tokenizer   = kwargs.get("tokenizer")        # HFìš©
    output_dir  = kwargs["output_dir"]           # ì €ì¥ ê²½ë¡œ (íŒŒì¼ ê²½ë¡œ)

    responses = {"WVS": {category: [] for category in data["WVS"].keys()}}

    for category, items in data["WVS"].items():
        for item in items:
            item_copy = item.copy()
            item_copy["Responses"] = []
            responses["WVS"][category].append(item_copy)

    os.makedirs(os.path.dirname(output_dir) or ".", exist_ok=True)

    for category, items in tqdm(responses["WVS"].items()):
        for item in items:
            try:
                if 'gpt' in model_path.lower():
                    response = gpt_chat(client, item, mode, model_path)
                else:
                    response = hf_chat(model, tokenizer, item, mode)

                item["Responses"] = response

            except Exception as e:
                print(f"Error with item {item.get('Q_index','?')} at category {category}: {e}")

        with open(output_dir, "w", encoding="utf-8") as f:
            json.dump(responses, f, ensure_ascii=False, indent=4)

        print(f"ğŸ’š Saved intermediate results after category '{category}' to {output_dir}")

    print(f"ğŸ’™ Final results saved for {model_path} at {output_dir}")

    return responses

# ================== 2. Arguments ==================
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True, help="Model name to run survey with")
parser.add_argument("--prompt_mode", type=str, required=True, help="default or korea")
parser.add_argument("--key", type=str, required=False, help='Hugging Face key or OpenAI key')
parser.add_argument("--dataset_path", type=str, required=True, help='Survey Dataset Path')
args = parser.parse_args()

model_path = args.model_path
mode = args.prompt_mode
key = args.key
dataset_path = args.dataset_path

# ================== 3. Data & Model ==================
# Evaluation Data
with open(dataset_path, "r", encoding="utf-8") as f: data = json.load(f)

# Model Load
print(f'Mode: {mode}')
if 'gpt' in model_path.lower(): client = model_load(key, model_path)
else: tokenizer, model = model_load(key, model_path)

# ================== 4. Run ==================
safe_model = model_path.replace("/", "__")
output_dir=f"outputs/{safe_model}/kovalplus_responses_{mode}.json"

if 'gpt' in model_path.lower():
    run_survey(
        data=data,
        model_path=model_path,
        client=client,
        mode=mode,
        output_dir=output_dir,
    )
else: 
    run_survey(
        data=data,
        model_path=model_path,
        model=model,
        tokenizer=tokenizer,
        mode=mode,
        output_dir=output_dir,
    )

# ================== 5. Eval - Measuring similarity: Korean vs. model responses ==================
from eval import run as eval_run

with open(output_dir, "r", encoding="utf-8") as f: survey_data = json.load(f)
eval_run(data=survey_data, out_dir=f"outputs/{safe_model}/", mode=mode)